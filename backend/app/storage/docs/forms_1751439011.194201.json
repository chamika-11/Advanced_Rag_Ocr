{
  "document_type": "forms",
  "raw_text": "Chapter 3\n\nDeep learning: basics and\nconvolutional neural networks\n\n(CNN)\n\nMaria Vakalopoulou!, Stergios Christodoulidis!,\nNinon Burgos’, Olivier Colliot?, and Vincent\nLepetit®\n\nl'Université Paris-Saclay, CentraleSupélec, Mathématiques et Informatique\npour la Complexité et les Systémes, 91190, Gif-sur-Yvette, France.\n?Sorbonne Université, Institut du Cerveau - Paris Brain Institute - ICM,\nCNRS, Inria, Inserm, AP-HP, H6pital de la Pitié-Salpétriére, F-75013, Paris,\nFrance\n\n3LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, France.\n“Corresponding author: e-mail address:\nmaria.vakalopoulou@centralesupelec.fr\n\nAbstract\n\nDeep learning belongs to the broader family of machine learning methods\nand currently provides state-of-the-art performance in a variety of fields,\nincluding medical applications. Deep learning architectures can be cate-\ngorized into different groups depending on their components. However,\nmost of them share similar modules and mathematical formulations. In\nthis chapter, the basic concepts of deep learning will be presented to\nprovide a better understanding of these powerful and broadly used al-\ngorithms. The analysis is structured around the main components of\ndeep learning architectures, focusing on convolutional neural networks\nand autoencoders.\n\nKeywords: perceptrons, backpropagation, convolutional neural\nnetworks, deep learning, medical imaging\n\nTo appear in\nO. Colliot (Ed.), Machine Learning for Brain Disorders, Springer\n\n\nVakalopoulou et al.\n\n1. Introduction\n\nRecently, deep learning frameworks have become very popular, attract-\ning a lot of attention from the research community. These frameworks\nprovide machine learning schemes without the need for feature engineer-\ning, while at the same time they remain quite flexible. Initially developed\nfor supervised tasks, they are nowadays extended to many other settings.\nDeep learning, in the strict sense, involves the use of multiple layers of\nartificial neurons. The first artificial neural networks were developed in\nthe late fifties with the presentation of the perceptron [1] algorithms.\nHowever, limitations related to the computational costs of these algo-\nrithms during that period, as well as the often-miscited claim of Minsky\nand Papert [2] that perceptrons are not capable of learning nonlinear\nfunctions such as the XOR, caused a significant decline of interest for\nfurther research on these algorithms and contributed to the so-called ar-\ntificial intelligence winter. In particular, in their book [2] Minsky and\nPapert discussed that single-layer perceptrons are only capable of\ning linearly separable patterns. It was often incorrectly believed that they\nalso presumed this is the case for multilayer perceptron networks. It took\nmore than ten years for research on neural networks to recover, and in [3\nsome of these issues were clarified and further discussed. Even if during\nthis period there was not a lot of research interest for perceptrons, very\nimportant algorithms such as the backpropagation algorithm [4, 5, 6, 7\nand recurrent neural networks [8] were introduced.\n\nearn-\n\nAfter this period, and in the early 2000s, publications by Hinton,\n\nOsindero, and Teh\nceptrons layer by lay\nBoltzmann machine\n\n9] indicated efficient ways to train multilayer per-\ner, treating each layer as an unsupervised restricted\nand then use supervised backpropagation for the\n\nfine-tuning [10]. Such advances in the optimization algorithms and in\n\nhardware in particu\n\nar graphics processing units (GPUs), increased the\n\ncomputational spee\neasier and faster. M\nwith ImageNet [11]\n\nable, contributing to\n\nof deep learning systems and made their training\noreover, around 2010, the first large-scale datasets,\nbeing one of the most popular, were made avail-\nthe success of deep learning algorithms, allowing\n\nthe experimental demonstration of their superior performance on several\n\ntasks in comparison\nrithms. Finally, ano\n\ncurrent popularity of\n\nwith other commonly used machine learning algo-\nher very important factor that contributed to the\neep learning techniques is their support by pub-\n\nlicly available and easy-to-use libraries such as Theano [12], Caffe [13],\n\nTensorflow [14], Keras\n\nthese publicly availa\nducible research and\n\n15] and Pytorch [16]. Indeed, currently, due to all\nle libraries that facilitate collaborative and repro-\naccess to resources from large corporations such as\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN 3\n\nKaggle, Google Colab, and Amazon Web Services, teaching and research\nabout these algorithms have become much easier.\n\nThis chapter will focus on the presentation and discussion of the main\ncomponents of deep learning algorithms, giving the reader a better under-\nstanding of these powerful models. The chapter is meant to be readable\nby someone with no background in deep learning. The basic notions of\nmachine learning will not be included here, however, the reader should\nrefer to Chapter 2 (reader without a background in engineering or com-\nputer science can also refer to Chapter 1 for a lay audience-oriented\npresentation of these concepts). The rest of this chapter is organized as\nfollows. We will first present the deep feedforward networks focusing on\nperceptrons, multilayer perceptrons, and the main functions that they are\ncomposed of (Section 2). Then, we will focus on the optimization of deep\nneural networks, and in particular, we will formally present the topics of\ngradient descent, backpropagation as well as the notions of generaliza-\ntion and overfitting (Section 3). Section 4 will focus on convolutional\nneural networks discussing in detail the basic convolutional operations,\nwhile Section 5 will give an overview of the autoencoder architectures.\n\n2. Deep feedforward networks\n\nIn this section, we will present the early deep learning approaches to-\ngether with the main functions that are commonly used in deep feed-\nforward networks. Deep feedforward networks are a set of parametric,\nnon-linear, and hierarchical representation models that are optimized\nwith stochastic gradient descent. In this definition, the term parametric\nholds due to the parameters that we need to learn during the training\nof these models, the non-linearity due to the non-linear functions that\nthey are composed of, and the hierarchical representation due to the fact\nthat the output of one function is used as the input of the next in a\nhierarchical way.\n\n2.1 Perceptrons\n\nThe perceptron [1] was originally developed for supervised binary classi-\nfication problems, and it was inspired by works from neuroscientists such\nas Donald Hebb [17]. It was built around a non-linear neuron, namely\nthe McCulloch-Pitts model of a neuron. More formally, we are looking\nfor a function f(x; w,b) such that f(.;w,b) : a € R’ > {+1,—1} where\nw and b are the parameters of f and the vector x = [21,...,2p]' is the\ninput. The training set is {(@),y)}. In particular, the perceptron\nrelies on a linear model for performing the classification:\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n4 Vakalopoulou et al.\n\n+1 ifw'e+bd>0\n—1 otherwise\n\nf(x; w,b) = (1)\n\nSuch a model can be interpreted geometrically as a hyperplane that\ncan appropriately divide data points that are linearly separable. More-\nover, one can observe that, in the previous definition, a perceptron is a\ncombination of a weighted summation between the elements of the input\nvector # combined with a step function that performs the decision for\nthe classification. Without loss of generality, this step function can be\nreplaced by other activation functions such as the sigmoid, hyperbolic\ntangent or softmax functions (see Section 2.3); the output simply needs\nto be thresholded to assign the +1 or —1 class. Graphically, a percep-\ntron is presented in Figure 1 on which each of the elements of the input\nis described as a neuron, and all the elements are combined by weighting\nwith the models’ parameters and then passed to an activation function\nfor the final decision.\n\neo >\n\nop 0-0\neo\n\nFigure 1: A simple perceptron model. The input elements are described\nas neurons and combined for the final prediction y. The final prediction\nis composed of a weighted sum and an activation function.\n\nDuring the training process and similarly to the other machine learn-\ning algorithms, we need to find the optimal parameters w and b for the\nperceptron model. One of the main innovations of Rosenblatt was the\nproposition of the learning algorithm using an iterative process. First,\nthe weights are initialized randomly, and then using one sample (a, y)\nof the training set the prediction of the perceptron is calculated. If the\nprediction is correct, no further action is needed, and the next data point\nis processed. If the prediction is wrong, the weights are updated with the\nfollowing rule: the weights are increased in case the prediction is smaller\nthan the ground truth label y® and decreased if the prediction is higher\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN 5\n\nthan the ground truth label. This process is repeated until no further\nerrors are made for the data points. A pseudocode of the training or\nconvergence algorithm is presented in the diagram 1 (note that in this\nversion, it is assumed that the data is linearly separable).\n\nAlgorithm 1 Train perceptron\n\nprocedure TRAIN({(2, y)})\nInitialization: initialize randomly the weights w and bias b\nwhile Ji € {1,...,n}, f(a; w,b) 4 y do\nPick i randomly\nerror = y — f(a; w, b)\nif error 4 0 then\nwe wterror: 2\nb< b+error\n\nOriginally, the perceptron has been proposed for binary classifica-\ntion tasks. However, this algorithm can be generalized for the case of\nmulticlass classification: f.(a;w,b) where c € {1,...,C} are the different\nclasses. This can be easily achieved by adding more neurons to the output\nlayer of the perceptron. That way, the number of output neurons would\nbe the same as the number of possible outputs we need to predict for the\nspecific problem. Then, the final decision can be made by choosing the\n\nmaximum of the different output neurons f, = Hay f(a; w, bd).\nc€{1,...,C\n\nFinally, in the following, we will integrate the bias b in the weights w\n(and thus add 1 as the first element of the input vector x = [1,71,...,2p]\").\nThe model can then be rewritten as f(a;w) such that f(.;w) : @ €\nRP+) > {41,-1}.\n\n2.2 Multilayer perceptrons\n\nThe limitation of perceptrons to linear problems can be overcome by\nusing multilayer perceptions, often denoted as MLP. An MLP consists of\nat least three layers of neurons: the input layer, a hidden layer, and an\noutput layer. Except for the input neurons, each neuron uses a non-linear\nactivation function, making it capable of distinguishing data that is not\nlinearly separable. These layers can also be called fully connected layers\nsince they connect all the neurons of the previous and of the current\nlayer. It is absolutely crucial to keep in mind that non-linear functions\nare necessary for the network to find non-linear separations in the data\n(otherwise, all the layers could simply be collapsed together into a single\ngigantic linear function).\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n6 Vakalopoulou et al.\n\n2.2.1. A simple multilayer network\n\nWithout loss of generality, an MLP with one hidden layer can be defined\nas:\n2(e) = g(W'e) 0\n9 = f(a; W, W*) = W?2z(2)\n\nwhere g(x) : R — R denotes the non-linear function (which can be\n\napplied element-wise to a vector), W+ the matrix of coefficients of the\n\nfirst layer and W? the matrix of coefficients of the second layer.\nEquivalently, one can write:\n\ndy\nGe = D> Wi. g(Wi) 2) , (3)\nj=l\n\nwhere d; is the number of neurons for the hidden layer which defines the\nwidth of the network, Wi denotes the first column of the matrix W'\nand Wi) denotes the c,j element of the matrix W?. Graphically, a\ntwo-layer perceptron is presented in Figure 2 on which the input neurons\nare fed into a hidden layer whose neurons are combined for the final\nprediction.\n\nFigure 2: An example of a simple multilayer perceptron model. The\ninput layer is fed into a hidden layer (z), which is then combined for the\nlast output layer providing the final prediction.\n\nThere were a lot of research works indicating the capacity of feedfor-\nward neural networks with a single hidden layer of finite size to ap-\nproximate continuous functions. In the late 80s, the first proof was\npublished [18] for sigmoid activation functions (see Section 2.3 for the\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN 7\n\ndefinition) and was generalized to other functions for feedforward mul-\ntilayer architectures [19, 20, 21]. In particular, these works prove that\nany continuous function can be approximated under mild conditions as\nclosely as wanted by a three-layer network. As N — oo, any continu-\nous function f can be approximated by some neural network f , because\neach component g(Wi) behaves like a basis function and functions in\na suitable space admits a basis expansion. However, since NV may need to\nbe very large, introducing some limitations for these types of networks,\ndeeper networks, with more than one hidden layer, can provide good\nalternatives.\n\n2.2.2. Deep neural network\n\nThe simple MLP networks can be generalized to deeper networks with\nmore than one hidden layer that progressively generate higher-level fea-\ntures from the raw input. Such networks can be written as\n\n21(a) = g(W*x)\nzn (x) = g(W*zn1(x)) : (4)\n\n9 = f(a; W',...,W*) = zx (2K -1(..-(21(@))))\n\nwhere K denotes the number of layers for the neural network, which\ndefines the depth of the network. In Figure 3, a graphical representation\nof the deep multilayer perceptron is presented. Once again, the input\nlayer is fed into the different hidden layers of the network in a hierarchical\nway such that the output of one layer is the input of the next one. The\nlast layer of the network corresponds to the output layer, which makes\nthe final prediction of the model.\n\nAs for networks with one hidden layer, they are also universal ap-\nproximators. However, the approximation theory for deep networks is\nless understood compared with neural networks with one hidden layer.\nOverall, deep neural networks excel at representing the composition of\nfunctions.\n\nSo far, we have described neural networks as simple chains of layers,\napplied in a hierarchical way, with the main considerations being the\ndepth of the network (the number of layers A’) and the width of each\nk layer (the number of neurons d,). Overall, there are no rules for the\nchoice of the AK and d; parameters that define the architecture of the\nMLP. However, it has been shown empirically that deeper models per-\nform better. In Figure 4, an overview of two different networks with 3\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n8 Vakalopoulou et al.\n\nFigure 3: An example of a deep neural network. The input layer, the\nk” layer of the deep neural network, and the output layer are presented\nin the figure.\n\nand 11 hidden layers is presented with respect to the number of parame-\nters and their accuracy. For each architecture, the number of parameters\nvaries by changing the number of neurons d;. One can observe that,\nempirically, deeper networks achieve better performance using approxi-\nmately the same or a lower number of parameters. Additional evidence to\nsupport these empirical findings is a very active field of research [22, 23].\n\nAccuracy 11 layers 2 networks with almost\nthe same number of\nparameters, but\ndifferent depths and\n\ndifferent accuracies\no—____“_.\n\n3 layers\n\nNumber of parameters\n\nFigure 4: Comparison of two different networks with almost the same\nnumber of parameters, but different depths. Figure inspired by [24].\n\nNeural networks can come in a variety of models and architectures.\nThe choice of the proper architecture and type of neural network depends\non the type of application and the type of data. Most of the time, the best\narchitecture is defined empirically. In the next section, we will discuss\nthe main functions used in neural networks.\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN 9\n\n2.3 Main functions\n\nA neural network is a composition of different functions also called mod-\nules. Most of the times, these functions are applied in a sequential way.\nHowever, in more complicated designs (e.g. deep residual networks),\ndifferent ways of combining them can be designed. In the following sub-\nsections, we will discuss the most commonly used functions that are the\nbackbones of most perceptrons and multilayer perceptron architectures.\nOne should note, however, that a variety of functions can be proposed\nand used for different deep learning architectures with the constraint to\nbe differentiable -almost- everywhere. This is mainly due to the way that\ndeep neural networks are trained and this will be discussed later in the\nchapter.\n\n2.3.1. Linear functions\n\nOne of the most fundamental functions used in deep neural networks is\nthe simple linear function. Linear functions produce a linear combination\nof all the nodes of one layer of the network, weighted with the param-\neters W. The output signal of the linear function is Wa, which is a\npolynomial of degree one. While it is easy to solve linear equations, they\nhave less power to learn complex functional mappings from data. This\nis why they need to be combined with non-linear functions, also called\nactivation functions (the name activation has been initially inspired by\nbiology as the neuron will be active or not depending on the output of\nthe function).\n\nBox 1: Function Counting Theorem\n\nThe so-called Function Counting Theorem (Cover, 1965 {25]) counts\nthe number of linearly separable dichotomies of n points in gen-\neral position in R?. The theorem shows that, out of the total 2”\n\ndichotomies, only C(n,p) = 2-0 (\"; ‘) are homogeneously,\n\nlinearly separable.\n\nWhen n >> p, the probability of a dichotomy to be linearly sep-\narable converges to zero. This indicates the need for the integration\nof non-linear functions into our modeling and architecture design.\nNote that n >> p is a typical regime in machine learning and deep\nlearning applications where the number of samples is very large.\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n10 Vakalopoulou et al.\n\n2.3.2. Non-linear functions\n\nOne of the most important components of deep neural networks is the\nnon-linear functions, also called activation functions. They convert the\nlinear input signal of a node into non-linear outputs to facilitate the\nlearning of high-order polynomials. There are a lot of different non-\nlinear functions in the literature. In this subsection, we will discuss the\nmost classical non-linearities.\n\n5 at ~~ 5 5 5\n\nSS ee rs\n(A) Tanh (B) Sigmoid (C) ReLU\n\nFigure 5: Overview of different non-linear functions (in green) and their\nfirst order derivative (blue). (A) Hyperbolic tangent function (tanh), (B)\nSigmoid, (C) Rectified linear unit (ReLU).\n\nHyperbolic tangent function (tanh) One of the most standard non-\nlinear functions is the hyperbolic tangent function, a.k.a., the tanh func-\ntion. Tanh is symmetric around the origin with a range of values varying\nfrom —1 to 1. The biggest advantage of the tanh function is that it\nproduces a zero-centered output (Figure 5(A)), thereby supporting the\nbackpropagation process that we will cover in the next section. The tanh\nfunction is used extensively for the training of multilayer neural networks.\nFormally, the tanh function, together with its gradient, is defined as:\n\neX —e-#\n\ng = tanh(a)\n\noo = 1—tanh?(a)\n\ner bie -® : (5)\n\nOne of the downsides of tanh is the saturation of gradients that occurs\nfor large or small inputs. This can slow down the training of the networks.\n\nSigmoid Similarly to tanh, the sigmoid is one of the first non-linear\nfunctions that were used to compose deep learning architectures. One\nof the main advantages is that it has a range of values varying from 0\nto 1 (Figure 5(B)) and therefore is especially used for models that aim\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN 11\n\nto predict a probability as an output. Formally, the sigmoid function,\ntogether with its gradient, is defined as:\n\n1\n\n§=O(%) = 7s . (6)\n\n28 — o(@)(1 — o(@)\n\nNote that this is in fact the logistic function, which is a special case\nof the more general class of sigmoid function. As it is indicated in Fig-\nure 5(B), the sigmoid gradient vanishes for large or small inputs making\nthe training process difficult. However, in case it is used for the output\nunits which are not latent variables and on which we have access to the\nground truth labels, sigmoid may be a good option.\n\nRectified Linear Unit (ReLU) ReLU is considered among the de-\nfault choice of non-linearity. Some of the main advantages of ReLU in-\nclude its efficient calculation and better gradient propagation with fewer\nvanishing gradient problems compared to the previous two activation\nfunctions [26]. Formally, the ReLU function, together with its gradient,\nis defined as:\n\ng = max(0, x)\ndg  f0ife<0 . (7\ndx) 1ife>0\n\nAs it is indicated in Figure 5(C), ReLU is differentiable anywhere else\nthan zero. However, this is not a very important problem as the value of\nthe derivative at zero can be arbitrarily chosen to be 0 or 1. In [27] the\nauthors empirically demonstrated that the number of iterations requirec\nto reach 25% training error on the CIFAR-10 dataset for a four-layer\nconvolutional network was six times faster with ReLU than with tanh\nneurons. On the other hand, and as discussed in [28], ReLU type neura\nnetworks which yield a piecewise linear classifier function produce al-\nmost always high confidence predictions far away from the training data.\nHowever, due to its efficiency and popularity, many variations of ReLU\nhave been proposed in the literature, such as the leaky ReLU [29] or the\nparametric ReLU [30]. These two variations both address the problem o\ndying neurons, where some ReLU neurons die for all inputs and remain\ninactive no matter what input is supplied. In such a case, no gradient\nflows from these neurons, and the training of the neural network architec-\nture is affected. Leaky ReLU and parametric ReLU change the g(x) = 0\npart, by adding a slope and extending the range of ReLU.\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n12 Vakalopoulou et al.\n\nSwish The choice of the activation function in neural networks is not\nalways easy and can greatly affect performance. In [31], the authors per-\nformed a combination of exhaustive and reinforcement learning-based\nsearches to discover novel activation functions. Their experiments dis-\ncovered a new activation function that is called Swish, and is defined\nas:\n\ng=ax-o(Gax)\na (8)\nAg = G9l@) + o(Bx)(1— Bg(x))\nOx\nwhere a is the sigmoid function and ( is either a constant or a trainable\nparameter. Swish tends to work better than ReLU on deeper models, as\nit has been shown experimentally in [31] in different domains.\n\nSoftmax Softmax is often used as the last activation function of a\nneural network. In practice, it normalizes the output of a network to\na probability distribution over the predicted output classes. Softmax is\ndefined as:\n\nc\ne;\n\nSoftmax(2x;) = oe: (9)\ngg\n\nThe softmax function takes as input a vector x of C’ real numbers\nand normalizes it into a probability distribution consisting of C proba-\nbilities proportional to the exponentials of the input numbers. However,\na limitation of softmax is that it assumes that every input x belongs to\nat least one of the C classes (which is not the case in practice, i.e. the\nnetwork could be applied to an input that does not belong to any of the\nclasses).\n\n2.3.3. Loss functions\n\nBesides the activation functions, the loss function (which defines the cost\nfunction) is one of the main elements of neural networks. It is the func-\ntion that represents the error for a given prediction. To that purpose,\nfor a given training sample, it compares the prediction f(a; W) to the\nground truth y® (here we denote for simplicity as W all the parame-\nters of the network, combining all the W1,...,W* in the multilayer\nperceptron shown above). The loss is denoted as ¢(y, f(a@;W)). The\naverage loss across the training samples (n) is called the cost function\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN 13\n\nand is defined as:\nJW) =-S 2 e(y, fe; W)) , (10)\n\nwhere {a, yO hin 1.n composes the training set. The aim of the training\nwill be to find the parameters W such that J(W) is minimized. Note\nthat, in deep learning, one often calls the cost function the loss function,\nalthough, strictly speaking, the loss is for a given sample, and the cost\nis averaged across samples. Besides, the objective function is the over-\nall function to minimize, including the cost and possible regularization\nterms. However, in the remainder of this chapter, in accordance with\ncommon usage in deep learning, we will sometimes use the term loss\nfunction instead of cost function.\n\nIn neural networks, the loss function can be virtually any function\nthat is differentiable. Below we present the two most common losses,\nwhich are respectively used for classification or regression problems. How-\never, specific losses exist for other tasks, such as segmentation, which are\ncovered in the corresponding chapters.\n\nCross entropy loss One of the most basic loss functions for classifi-\ncation problems corresponds to the cross-entropy between the expected\nvalues and the predicted ones. It leads to the following cost function:\n\nJ(W)= — Flog (P(y =y®|x=29,W)) , (11)\n\nwhere P (y = yO|x = 2, Ww) is the probability that a given sample is\ncorrectly classified.\n\nThe cross-entropy can also be seen here as the negative log-likelihood\nof the training set given the predictions of the network. In other words,\nminimizing this loss function corresponds to maximizing the likelihood:\n\nL(W) =][ PW =y9|x=29W) . (12\n71\n\nMean squared error loss For regression problems, the mean squarec\nerror is one of the most basic cost functions, measuring the average of the\nsquares of the errors, which is the average squared difference between the\npredicted values and the real ones. The mean squared error is definec\nas:\n\nJW) = So lly — f@;W)IP . (13\ni=l\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n14 Vakalopoulou et al.\n\n3. Optimization of deep neural networks\n\nOptimization is one of the most important components of neural net-\nworks, and it focuses on finding the parameters W that minimize the\nloss function J(W). Overall, optimization is a difficult task. Tradition-\nally, the optimization process is performed by carefully designing the loss\nfunction and integrating its constraints to ensure that the optimization\nprocess is convex (and thus, one can be sure to find the global mini-\nmum). However, neural networks are non-convex models, making their\noptimization challenging and, in general, one does not find the global\nminimum but only a local one. In the next sections, the main compo-\nnents of their optimization will be presented, giving a general overview\nof the optimization process, its challenges, and common practices.\n\n3.1 Gradient descent\n\nGradient descent is an iterative optimization algorithm that is among\nthe most popular and basic algorithms in machine learning. It is a first-\norder! optimization algorithm, which is finding a local minimum of a\ndifferentiable function. The main idea of gradient descent is to take iter-\native steps toward the opposite direction of the gradient of the function\nthat needs to be optimized (Figure 6).\n\nJ(W)\n\n_ad(W)\nan\n\n&\n/\n\nWwW\n\nFigure 6: The gradient descent algorithm. This first-order optimization\nalgorithm is finding a local minimum by taking steps toward the opposite\ndirection of the gradient.\n\n'First-order means here that the first-order derivatives of the cost function are\nused as opposed to second-order algorithms that, for instance, use the Hessian.\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN 15\nThat way, the parameters W of the model are updated by\nOJ(W'*)\nt+1 t 14\nwr-w—n awe (14)\n\nwhere t is the iteration and 7, called learning rate, is the hyperparameter\nthat indicates the magnitude of the step that the algorithm will take.\nBesides its simplicity, gradient descent is one of the most commonly\nused algorithms. More sophisticated algorithms require computing the\nHessian (or an approximation), and/or its inverse (or an approximation).\n\nEven if these varia\n\ntions could give better optimization guarantees, they\n\nare often more computationally expensive, making gradient descent the\n\ndefault method for\n\nIn the case of\nreduced to the pro\nis then guaranteec\nidentify it. Howev\nneural networks, it\n\nof gradient descent\nidentifiable [24]. A model is said to be identifiable i\n\npossible, given a s\nset of the model’s\n\noptimization.\nconvex functions, the optimization problem can be\npblem of finding a local minimum. Any local minimum\nto be a global minimum and gradient descent can\ner, when dealing with non-convex functions, such as\nis possible to have many local minima making the use\nchallenging. Neural networks are, in general, non-\n' it is theoretically\nufficiently large training set, to rule out all but one\narameters. Models with latent variables, such as the\n\nhidden layers of neural networks, are often not iden’\n\n‘ifiable because we\n\ncan obtain equivalent models by exchanging latent variables with each\n\nother. However, all these minima are often almost\n\nequivalent to each\n\nother in cost function value. In that case, these local minima are not a\nproblematic form of non-convexity. It remains an open question whether\n\nthere exist many local minima with a high cost tha\ntraining of neural networks. However, it is currently\n\nprevent adequate\nbelieved that most\n\nlocal minima, at least as found by modern optimization procedures, will\n\ncorrespond to a\n\now cost (even though not to identical costs) [24].\n\nFor W* to be a local minimum, we need mainly two conditions to be\n\nfulfilled\ne all the eigenvalues of (a (w*)) to be positive.\nFor random\n\nunctions in n dimensions, the probability for the eigen-\n\nvalues to be all positive is i On the other hand, the ratio of the number\n\nof sadd\n\nle points to local minima increases exponentially with n [32]. A\n\nsaddle point, or critical point, is a point where the derivatives are zero\nwithout being a minimum of the function. Such points could result in\na high error making the optimization with gradient descent challenging.\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n16 Vakalopoulou et al.\n\nIn [32], this issue is discussed, and an optimization algorithm that lever-\nages second-order curvature information is proposed to deal with this\nissue for deep and recurrent networks.\n\n3.1.1. Stochastic gradient descent\n\nGradient descent efficiency is not enough when it comes to machine learn-\ning problems with large numbers of training samples. Indeed, this is the\ncase for neural networks and deep learning which often rely on hundreds\nor thousands of training samples. Updating the parameters W after\ncalculating the gradient using all the training samples would lead to a\ntremendous computational complexity of the underlying optimization al-\ngorithm [33]. To deal with this problem, the stochastic gradient descent\n(SGD algorithm) is a drastic simplification. Instead of computing the\noL(W) exactly, each iteration estimates this gradient on the basis of a\n\naw\nsmall set of randomly picked examples, as follows:\n\nwi. w'—nG(Ww'), (15)\nwhere\neq = Lr 1, f(a; Ww) (16)\n\nwhere {(a), y“))\\,-1.«¢ is the small subset of K training samples\n(Kk << N). This subset of K samples is called a mini-batch or some-\ntimes a batch”. In such a way, the iteration cost of stochastic gradient\ndescent will be O(/<) while for gradient descent O(N). The ideal choice\nfor the batch size is a debated question. First, an upper limit for the\nbatch size is often simply given the available GPU memory, in particu-\nlar when the size of the input data is large (e.g. 3D medical images).\nBesides, choosing K as a power of 2 often leads to more efficient compu-\ntations. Finally, small batch sizes tend to have a regularizing effect which\ncan be beneficial [24]. In any case, the ideal batch size usually depends\non the application, and it is not uncommon to try different batch sizes.\nFinally, one calls an epoch a complete pass over the whole training set\n(meaning that each training sample has been used once). The number of\nepochs is the number of full passes over the whole training set. It should\nnot be confused with the number of iterations which is the number of\nmini-batches that have been processed.\n\n2Note that, as often in deep learning, the terminology can be confusing. In iso-\nlation, the term batch is usually a synonym of mini-batch. On the contrary, batch\ngradient descent means computing the gradient using all training samples and not\nonly a mini-batch [24].\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN 17\n\nNote that various improvements over traditional SGD have been in-\ntroduced, leading to more efficient optimization methods. These state-\nof-the-art optimization methods are presented in Section 3.4.\n\nBox 2: Convergence of SGD theorem\n\nIn [34], the authors prove that stochastic gradient descent converges\nif the network is sufficiently overparametrized. Let (2, y) cren\nbe a training set satisfying min, ji; ||[e@ — 2 ||) > 5 > 0. Con-\nsider fitting the data using a feedforward neural network with ReLU\nactivations. Denote by D (resp. W) the depth (resp. width) of\nthe network. Suppose that the neural network is sufficiently over-\nparametrized, i.e.,\n\n1\nW > polynomial(n, D, 5) . (17)\nThen, with high probability, running SGD with some random ini-\ntialization and properly chosen step sizes m yields J(W‘) < € in\nt x log 4.\n\n3.2 Backpropagation\n\nThe training of neural networks is performed with backpropagation. Back-\npropagation computes the gradient of the loss function with respect to\nthe parameters of the network in an efficient and local way. This algo-\nrithm was originally introduced in 1970. However, it started becoming\nvery popular after the publication of [6], which indicated that backprop-\nagation works faster than other methods that had been proposed back\nthen for the training of neural networks.\n\nWy W2\no———_-' &—— J(W)\n\nFigure 7: A multilayer perceptron with one hidden layer.\n\nThe backpropagation algorithm works by computing the gradient of\nthe loss function (J) with respect to each weight by the chain rule, com-\nputing the gradient one layer at a time, and iterating backward from\nthe last layer to avoid redundant calculations of intermediate terms in\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n18 Vakalopoulou et al.\n\nthe chain rule. In Figure 7, an example of a multilayer perceptron with\none hidden layer is presented. In such a network, the backpropagation is\ncalculated as\n\nas(W) _as(W) ag\n\nOw2 Og Owe (18)\nOJ(W) _ OJ(W) x Oy _ OJ(W x Og x eran\nOw, Og Ow, Og Oz =Ou,\n\nOverall, backpropagation is very simple and local. However, the rea-\nson why we can train a highly non-convex machine with many local min-\nima, like neural networks, with a strong local learning algorithm is not\nreally known even today. In practice, backpropagation can be computed\nin different ways, including manual calculation, numerical differentia-\ntion using finite difference approximation, and symbolic differentiation.\nNowadays, deep learning frameworks such as [14, 16] use automatic dif-\nferentiation [35] for the application of backpropagation.\n\n3.3 Generalization and overfitting\n\nSimilar to all the machine learning algorithms (discussed in Chapter\n2), neural networks can suffer from poor generalization and overfitting.\nThese problems are caused mainly by the optimization of the parameters\nof the models performed in the {(a;, yi) }in1..n training set, while we need\nthe model to perform well on other unseen data that are not available\nduring the training. More formally, in the case of cross entropy, the loss\nthat we would like to minimize is:\n\nJ(W)=-log T] Ply=ylx=2:W) , (19)\n(z,y)ETr\n\nwhere Ty is the set of any data, not available during training. In practice,\na small validation set Ty is used to evaluate the loss on unseen data. Of\ncourse, this validation set should be distinct from the training set. It is\nextremely important to keep in mind that the performance obtained on\nthe validation set is generally biased upwards because the validation set\nwas used to perform early stopping or to choose regularization parame-\nters. Therefore, one should have an independent test set, that has been\nisolated at the beginning, has not been used in any way during training,\nand is only used to report the performance (see Chapter 20 for details).\nIn case one cannot have an additional independent test set due to a lack\nof data, one should be aware that the performance may be biased and\nthat this is a limitation of the specific study.\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN 19\n\nTo avoid overfitting and improve the generalization performance of\nthe model, usually, the validation set is used to monitor the loss during\nthe training of the networks. Tracking the training and validation losses\nover the number of epochs is essential and provides important insights\ninto the training process and the selected hyperparameters (e.g. choice\nof learning rate). Recent visualization tools such as TensorBoard® or\nWeights & Biases! make this tracking easy. In the following, we will also\nmention some of the most commonly applied optimization techniques\nthat help with preventing overfitting.\n\nEarly stopping Using the reported training and validation errors, the\nbest model in terms of performance and generalization power is selected.\nIn particular, early stopping, which corresponds to selecting a model cor-\nresponding to an earlier time point than the final epoch is a common way\nto prevent overfitting [36]. Early stopping is a form of regularization for\nmodels that are trained with an iterative method, such as gradient de-\nscent and its variants. Early stopping can be implemented with different\ncriteria. However, generally, it requires the monitoring of the perfor-\nmance of the model on a validation set, and the model is selected when\nits performance degrades or its loss increases. Overall, early stopping\nshould be used almost universally for the training of neural networks [24].\nThe concept of early stopping is illustrated in Figure 8.\n\nLoss\n\nUnderfitting Overfitting Validation\n\nTraining\n\nTime (epochs)\n\nFigure 8: Illustration of the concept of early stopping. The model that\nshould be selected corresponds to the dashed bar which is the point where\nthe validation loss starts increasing. Before this point, the model is un-\nderfitting. After, it is overfitting.\n\n3https: //www.tensorf low. org/tensorboard\n4nttps://wandb.ai/site\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n20 Vakalopoulou et al.\n\nWeight regularization Similar to other machine learning methods\n(Chapter 2), weight regularization is also a very commonly used tech-\nnique for avoiding overfitting in neural networks. More specifically, dur-\ning the training of the model, the weights of the network start growing\nin size in order to specialize the model to the training data. However,\nlarge weights tend to cause sharp transitions in the different layers of the\nnetwork and, that way, large changes in the output for only small changes\nin the inputs [37]. To handle this problem, during the training process,\nthe weights can be updated in such a way that they are encouraged to be\nsmall, by adding a penalty to the loss function, for instance, the 2 norm\nof the parameters A||W||?, where A is a trade-off parameter between the\nloss and the regularization. Since weight regularization is quite popular\nin neural networks, different optimizers have integrated them into their\noptimization process in the form of weight decay.\n\nWeight initialization The way that the weights of neural networks\nwill be initialized is very important, and it can determine whether the\nalgorithm converges at all, with some initial points being so unstable that\nthe algorithm encounters numerical difficulties and fails altogether [24].\nMost of the time, the weights are initialized randomly from a Gaussian\nor uniform distribution. According to [24], the choice of Gaussian or\nuniform distribution does not seem to matter very much; however, the\nscale does have a large effect on both the outcome of the optimization\nprocedure and on the ability of the network to generalize. Nevertheless,\nmore tailored approaches have been developed over the last decade that\nhave become the standard initialization points. One of them is the Xavier\nInitaliazation [38] which balances between all the layers to have the same\nactivation variance and the same gradient variance. More formally the\nweights are initialized as\n\nW,,; ~ Uniform (-y 6 Af ) ; (20)\nm+n m+n\n\nwhere m is the number of inputs and n the number of outputs of matrix\nW. Moreover, the biases b are initialized to 0.\n\nDrop-out There are other techniques to prevent overfitting, such as\ndrop-out [39], which involves randomly destroying neurons during the\ntraining process, thereby reducing the complexity of the model. Drop-\nout is an ensemble method that does not need to build the models ex-\nplicitly. In practice, at each optimization iteration, random binary masks\non the units are considered. The probability of removing a unit (p) is\ndefined as a hyperparameter during the training of the network. During\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN 21\n\ninference, all the units are activated; however, the obtained parameters\nW are multiplied with this probability p. Drop-out is quite efficient and\ncommonly used in a variety of neural network architectures.\n\nData augmentation Since neural networks are data-driven methods,\ncheir performance depends on the training data. To increase the amount\nof data during the training, data augmentation can be performed. It gen-\nerates slightly modified copies of the existing training data to enrich the\nraining samples. This technique acts as a regularizer and helps reduce\noverfitting. Some of the most commonly used transformations applied\nduring data augmentation include random rotations, translations, crop-\nbing, color jittering, resizing, Gaussian blurring, and many more. In\nFigure 9, examples of different transformations on different digits (first\ncolumn) of the MNIST dataset [40] are presented. For medical images,\nche TorchIO library allows to easily perform data augmentation [41].\n\nO| O|o/O}0/9 lo |o|O/0/ 0) 0/0 | OO\nPi out tt | te ie Ne tae ah A\n2127.12 (| 212. (2/212 1212/2) 2/2 /2\n\n3 33 33\nVd NY des\nG G16 6 &\n8 Lf SF & 8\n\nFFIVIINPIAITZIVIUGP\n\nFigure 9: Examples of data transformations applied in the MNIST\ndataset. Each of these generated samples is considered additional train-\ning data.\n\nBatch normalization To ensure that the training of the networks will\nbe more stable and faster, batch normalization has been proposed [42].\nIn practice, batch normalization re-centers and re-scales the layer’s in-\nput, mitigating the problem of internal covariate shift which changes the\ndistribution of the inputs of each layer affecting the learning rate of the\nnetwork. Even if the method is quite popular, its necessity and use for\nthe training have recently been questioned [43].\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n22 Vakalopoulou et al.\n\n3.4 State-of-the-art optimizers\n\nOver the years, different optimizers have been proposed and widely used,\naiming to provide improvements over the classical stochastic gradient\ndescent. These algorithms are motivated by challenges that need to be\naddressed with stochastic gradient descent and are focusing on the choice\nof the proper learning rate, its dynamic change during training as well as\nthe fact that it is the same for all the parameter updates [44]. Moreover, a\nproper choice of optimizer could speed up the convergence to the optimal\nsolution. In this subsection, we will discuss some of the most commonly\nused optimizers nowadays.\n\n3.4.1. Stochastic gradient descent with momentum\n\nOne of the limitations of the stochastic gradient descent is that since the\ndirection of the gradient that we are taking is random, it can heavily\noscillate, making the training slower and even getting stuck in a saddle\npoint. To deal with this problem, stochastic gradient descent with mo-\nmentum [45, 46] keeps a history of the previous gradients, and it updates\nthe weights taking into account the previous updates. More formally,\n\ng' & pg * + (1— p)G(W')\nAW! + —mg! ; (21)\nwetew'+ aw\n\nwhere g’ is the direction of the update of the weights in time-step t\nand p € [0,1] is a hyperparameter that controls the contribution of the\nprevious gradients and current gradient in the current update. When\np = 0, it is the same as the classical stochastic gradient descent. A\nlarge value of ¢ will mean that the update is strongly influenced by the\nprevious updates.\n\nThe momentum algorithm accumulates an exponentially decaying\nmoving average of the past gradients and continues to move in their\ndirection [24]. Momentum increases the speed of convergence, while it\nis also helpful to not get stuck in places where the search space is flat\n(saddle points with zero gradient), since the momentum will pursue the\nsearch in the same direction as before the flat region.\n\n3.4.2. AdaGrad\n\nTo facilitate and speed up, even more, the training process, optimizers\nwith adaptive learning rates per parameter have been proposed. The\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN 23\n\nadaptive gradient (AdaGrad) optimizer [47] is one of them. It updates\neach individual parameter proportionally to their component (and mo-\nmentum) in the gradient. More formally,\n\ng — G(W’)\nrer tg ogi\n(22)\n\nAW: —-—\" _ogt’\nit+vn 9%\n\nwi! w'+aw'\n\nwhere g‘ is the gradient estimate vector in time-step ¢, r’ is the term\ncontrolling the per parameter update, and 6 is some small quantity that\nis used to avoid the division by zero. Note that r‘ constitutes of the\ngradient’s element-wise product with itself and of the previous term r‘~+\naccumulating the gradients of the previous terms.\n\nThis algorithm performs very well for sparse data since it decreases\nthe learning rate faster for the parameters that are more frequent and\nslower for the infrequent parameters. However, since the update accumu-\nlates gradients of the previous steps, the updates could decrease very fast,\nblocking the learning process. This limitation is mitigated by extensions\nof the AdaGrad algorithm as we discuss in the next sections.\n\n3.4.38. RMSProp\n\nAnother algorithm with adaptive learning rates per parameter is the root\nmean squared propagation (RMSProp) algorithm, proposed by Geoffrey\nHinton. Despite its popularity and use, this algorithm has not been\npublished. RMSProp is an extension of the AdaGrad algorithm dealing\nwith the problem of radically diminishing learning rates by being less\ninfluenced by the first iterations of the algorithm. More formally,\n\ng — G(Ww’)\n\nre pr +(1— pg og\nAWt a — ) © gt\nb+vn\nwt wi+aw\n\nwhere p is a hyperparameter that controls the contribution of the previous\ngradients and the current gradient in the current update. Note that\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n24 Vakalopoulou et al.\n\nRMSProp estimates the squared gradients in the same way as AdaGrad,\nbut instead of letting that estimate continually accumulate over training,\nwe keep a moving average of it, integrating the momentum. Empirically,\nRMSProp has been shown to be an effective and practical optimization\nalgorithm for deep neural networks [24].\n\n3.4.4. Adam\n\nThe effectiveness and advantages of the AdaGrad and RMSProp algo-\nrithms are combined in the adaptive moment estimation (Adam) opti-\nmizer [48]. The method computes individual adaptive learning rates for\ndifferent parameters from estimates of the first and second moments of\nthe gradients. More formally,\n\ngj — G(W’)\n\nsi & ps\" + (1 pig\n\nr & port! + (L— pr)g' Og!\n\nt\n\ngee\n1— (pi) . (24)\nPee oe\n1— (pe)!\nAWt + —\n\ndog\nb+ VR\nwee vz w'+aw'\n\nwhere sé is the gradient with momentum, r’ accumulates the squared\ngradients with momentum as in RMSProp, é and ?* are smaller than\ns’ and r* respectively but they converge towards them. Moreover, 6 is\nsome small quantity that is used to avoid the division by zero while p;\nand p2 are hyperparameters of the algorithm. The parameters p; and p2\ncontrol the decay rates of each moving average, respectively, and their\nvalue is close to 1. Empirical results demonstrate that Adam works\nwell in practice and compares favorably to other stochastic optimization\nmethods, making it the go-to optimizer for deep learning problems.\n\n3.4.5. Other optimizers\n\nThe development of efficient (in terms of speed and stability) optimizers\nis still an active research direction. RAdam [49] is a variant of Adam,\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN 25\n\nintroducing a term to rectify the variance of the adaptive learning rate.\nIn particular, RAdam leverages a dynamic rectifier to adjust the adaptive\nmomentum of Adam based on the variance and effectively provides an\nautomated warm-up custom-tailored to the current dataset to ensure a\nsolid start to training. Moreover, LookAhead [50] was inspired by recent\nadvances in the understanding of loss surfaces of deep neural networks\nand provides a breakthrough in robust and stable exploration during\nthe entirety of the training. Intuitively, the algorithm chooses a search\ndirection by looking ahead at the sequence of fast weights generated by\nanother optimizer. These are only some of the optimizers that exist in the\nliterature, and depending on the problem and the application, different\noptimizers could be selected and applied.\n\n4. Convolutional neural networks\n\nConvolutional neural networks (CNNs) are a specific category of deep\nneural networks that employ the convolution operation in order to pro-\ncess the input data. Even though the main concept dates back to the 90s\nand is greatly inspired by neuroscience [51] (in particular by the organi-\nzation of the visual cortex), their widespread use is due to a relatively\nrecent success on the ImageNet Large Scale Visual Recognition Challenge\nof 2012 [27]. In contrast to the deep fully-connected networks that have\neen already discussed, CNNs excel in processing data with a spatial or\ngrid-like organization (e.g., time series, images, videos, etc.), while at\nshe same time decrease the number of trainable parameters due to their\nweight sharing properties. The rest of this section is first introducing the\nconvolution operation and the motivation behind using it as a building\nlock/module of neural networks. Then, a number of different variations\nare presented together with examples of the most important CNN archi-\nectures. Lastly, the importance of the receptive field — a central property\nof such networks — will be discussed.\n\n4.1 The convolution operation\n\nThe convolution operation is defined as the integral of the product of\nthe two functions (f, g)° after one is reversed and shifted over the other\nfunction. Formally we write,\n\nh(t) = / “ Fer~ @atrie . (25)\n\n5Note that f and g have no relationship to their previous definitions in the chapter.\nIn particular, f is not the deep learning model.\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n26 Vakalopoulou et al.\n\nSuch an operation can also be denoted with an asterisk (*) so it is\nwritten as,\n\nh(t) = (f*g)(t) . (26)\n\nIn essence, the convolution operation shows how one function affects\nthe other. This intuition arises from the signal processing domain, where\nit is typically important to know how a signal will be affected by a filter.\nFor example, consider a uni-dimensional continuous signal, like the brain\nactivity of a patient on some electroencephalography electrode, and a\nGaussian filter. The result of the convolution operation between these\ntwo functions will output the effect of a Gaussian filter on this signal\nwhich will, in fact, be a smoothed version of the input.\n\nA different way to think of the convolution operation is that it shows\n\nhow the two functions are related. In other words, it shows how similar or\ndissimilar the two functions are at different relative positions. In fact, the\nconvolution operation is very similar to the cross-correlation operation,\nwith the subtle difference being that in the convolution operation, one of\nthe two functions is inverted. In the context of deep learning specifically,\nthe exact differences between the two operations can be of secondary\nconcern; however, the convolution operation has more properties than\ncorrelation, such as commutativity. Note also that when the signals are\nsymmetric both operations will yield the same result.\nIn order to deal with discrete and finite signals, we can expand the\ndefinition of the convolution operation. Specifically, given two discrete\nsignals f[k] and g[k], with k € Z, the convolution operation is defined\nby,\n\nlk] = So fle —nlgin) . (27)\n\nLastly, the convolution operation can be extended for multi-dimensional\nsignals similarly. For example, we can write the convolutional operation\nbetween two discrete and finite two-dimensional signals (e.g., I[¢, j], Ki, j])\nas\n\nAli, j] =O Mi -m, 7 — n]K [m,n] : (28)\n\nVery often, the first signal will be the input of interest (e.g. a large\nsize image) while the second signal will be of relatively small size (e.g.\na 3 x 3 or 4x 4 matrix) and will implement a specific operation. The\nsecond signal is then called a kernel. In Figure 10, a visualization of the\nconvolution operation is shown in the case of a 2D discrete signal such as\nan image and a 3 x 3 kernel. In detail, the convolution kernel is shifted\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN 27\n\nOle] Rl w]e\n\nFlOlO;O;/O;/O;oO\nSle lOlO;o;o|e\nOR RIO; O;Rie\n\nFigure 10: A visualization of the discrete convolution operation in 2D.\n\nover all locations of the input, and an element-wise multiplication and a\nsummation are utilized to calculate the convolution output at the corre-\nsponding location. Examples of applications of convolutions to an image\nare provided in Figure 11. Finally, note that, as in multilayer percep-\ntrons, a convolution will generally be followed by a non-linear activation\nfunction, for instance, a ReLU (see Figure 12 for an example of activation\napplied to a feature map).\n\nOriginal image Vertical edge detection Horizontal edge detection\n\nFigure 11: Two examples of convolutions applied to an image. One\nof the filters acts as a vertical edge detector, and the other one as a\nhorizontal edge detector. Of course, in CNNs, the filters are learned, not\npredefined, so there is no guarantee that, among the learned filters, there\nwill be a vertical/horizontal case detector, although it will often be the\ncase in practice, especially for the first layers of the architecture.\n\nIn the following sections of this chapter, any reference to the convolu-\ntion operation will mostly refer to the 2D discrete case. The extension to\nthe 3D case, which is often encountered in medical imaging, is straight-\nforward.\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n28 Vakalopoulou et al.\n\nRectified linear unit (ReLU) Input feature map Rectified feature map\n\ng(z) = max(0,z)\n\nFigure 12: Example of application of a non-linear activation function\n(here a ReLU) to an image.\n\n4.2 Properties of the convolution operation\n\nIn the case of a discrete domain, the convolution operation can be per-\nformed using a simple matrix multiplication without the need of shifting\none signal over the other one. This can be essentially achieved by uti-\nlizing the Toeplitz matrix transformation. The Toeplitz transformation\ncreates a sparse matrix with repeated elements which, when multiplied\nwith the input signal, produces the convolution result. To illustrate how\nthe convolution operation can be implemented as a matrix multiplication,\nlet’s take the example of a 3 x 3 kernel (A) and a 4 x 4 input (J)\n\ntoo tor %02 03\nkoo Kor Koz ho iss im i\nK=l|ko ki kyl and I= ‘e m in ri\n20 ta, ta t23\n\nog a1 keg coe ee\n\n130 «431 432 433\n\nThen, the convolution operation can be computed as a matrix multipli-\ncation between the Toepliz transformed kernel\n\nkoo kor kor 0 fio Air Aig 0 kao kor kop OF 0 0 O O\n0 koo kor kor O fio Au ig 0 hoo kor koe 0 0 O 0\n0 0 0 0 Koo kor hoo O kio ki hig O kyo kor kop 0\n0 0 0 0 0 koo kor ko2 O fio Kar kag 0 hoo kor koe\n\nK=\n\nand a reshaped input\nI= [too to: tor to3 to tr tie tig t20 tar toe ing igo tg1 ig2 tgs]\n\nThe produced output will need to be reshaped as a 2 x 2 matrix in\norder to retrieve the convolution output. This matrix multiplication\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN 29\n\nimplementation is quite illuminating on a few of the most important\nproperties of the convolutional operation. These properties are the main\nmotivation behind using such elements in deep neural networks.\n\nBy transforming the convolution operation to a matrix multiplication\noperation, it is evident that it can fit in the formalization of the linear\nfunctions, which has already been presented in section 2.3. As such, deep\nneural networks can be designed in a way to utilize trainable convolution\nkernels. In practice, multiple convolution kernels are learned at each con-\nvolutional block while several of these trainable convolutional blocks are\nstacked on top of each other forming deep CNNs. Typically, the output\nof a convolutional operation is called a feature map or just features.\n\nPOO® Ist hidden layer b@@@ 1st hidden layer\nee — O66 20d filter\n\nTHF HHH ese o es osy\nSHH es es oe os ree\nFH tHe sete sso oee\nHHH Fe Hes oeerooss,\nFH HH eso eseoeoeoss\n\nInput layer\n(image) 1st hidden layer\n1st feature map\n\n1st hidden layer\n2nd feature map\n\nFigure 13: For a given layer, several (usually many) filters are learned,\neach of them being able to detect a specific characteristic in the image,\nresulting in several feature/filter maps. On the other hand, for a given\nfilter, the weights are shared across all the locations of the image.\n\nAnother important aspect of the convolution operation is that it re-\nquires much fewer parameters than the fully connected MLP-based deep\nneural networks. As it can also be seen from the K matrix, the exact\nsame parameters are shared across all locations. Eventually, rather than\nlearning a different set of parameters for the different locations of the\ninput, only one set is learned. This is referred to as parameter sharing or\nweight sharing and can greatly decrease the amount of memory that is\nrequired to store the network parameters. An illustration of the process\nof weight sharing across locations, together with the fact that multiple\nfilters (resulting in multiple feature maps) are computed for a given layer,\nis illustrated in Figure 13. The multiple feature maps for a given layer\nare stored using another dimension (see Figure 14), thus resulting in a\n3D array when the input is a 2D image (and a 4D array when the input\nis a 3D image).\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n30 Vakalopoulou et al.\n\nDifferent feature maps are arranged\nsent” af along the depth\n\n1\n\nFigure 14: The different feature maps for a given layer are arranged\nalong another dimension. The feature maps will thus be a 3D array when\nthe input is a 2D image (and a 4D array when the input is a 8D image).\n\nConvolutional neural networks have proven quite powerful in pro-\ncessing data with spatial structure (e.g. images, videos, etc.). This is\neffectively based on the fact that there is a local connectivity of the\nkernel elements while at the same time the same kernel is applied at\ndifferent locations of the input. Such processing grants a quite useful\nproperty called translation equivariance enabling the network to output\nsimilar responses at different locations of the input. An example of the\nusefulness of such a property can be identified on an image detection\ntask. Specifically, when training a network to detect tumors in an MRI\nimage of the brain, the model should respond similarly regardless of the\nlocation where the anomaly can be manifested.\n\nLastly, another important property of the convolutional operation is\nthat it decouples the size of the input with the trainable parameters. For\nexample, in the case of MLPs the size of the weight matrix is a function\nof the dimension of the input. Specifically, a densely connected layer that\nmaps 256 features to 10 outputs would have a size of W € R!*>5, On\nthe contrary, in convolutional layers, the number of trainable parameters\nonly depends on the kernel size and the number of kernels that a layer\nhas. This eventually allows the processing of arbitrarily sized inputs, for\nexample, in the case of fully convolutional networks.\n\n4.3 Functions and variants\nAn observant reader might have noticed that the convolution operation\ncan change the dimensionality of the produced output. In the example\n\nvisualized in Figure 10, the image of size 7 x 7, when convolved with\na kernel of size 3 x 3, produces a feature map of size of 5 x 5. Even\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN 31\n\nFilter=parameters=weights Feature map\n\nImage\nFigure 15: The padding operation, which involves adding zeros around\nthe image, allows to obtain feature maps that are of the same size as the\noriginal image.\n\nInput feature map\nPooled feature map\n\nMax pooling with\n2x2 filter and stride 2\n\nFigure 16: Effect of a pooling operation. Here a Maximum Pooling of\nsize 2 x 2 with a stride of 2.\n\nthough dimension changes can be avoided with appropriate padding (see\nFigure 15 for an illustration of this process) prior to the convolution\noperation, in some cases it is actually desired to reduce the dimensions\nof the input. Such a decrease can be achieved in a number of ways\ndepending the task at hand. In this subsection, some of the most typical\nfunctions that are utilized in CNNs will be discussed.\n\nDownsampling operations (i.e. pooling layers) In many CNN\narchitectures there is an extensive use of downsampling operations that\naim to compress the size of the feature maps and decrease the computa-\ntional burden. Otherwise referred to as pooling layers, these processing\noperations are aggregating the values of their input depending on their\ndesign. Some of the most common downsampling layers are the Mavi-\nmum Pooling, Average Pooling, or Global Average Pooling. In the first\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n32 Vakalopoulou et al.\n\nOo Se ee oe\n$$ oo e+ SH sey\nFoor or oes Se\n+ te eH + > +\n+++ 4+ 4445\nHHH Hoe oese\nTHF HEoeoee\nTHHH HH HHeese\nnanan ananananananand\nSanaa ananananan anand\nSn aa an ana nan ananaina\n\nStride=2\n\nInput layer\n(image) 1st hidden layer\n\nFigure 17: Stride operation, here with a stride of 2.\n\ntwo, either the maximum or the average value is used as a feature for\nthe output across non overlapping regions of a predefined pooling size.\nIn the case of the global average pooling, the spatial dimensions are all\nrepresented with the average value. An example of pooling is provided\nin Figure 16.\n\nStrided convolution ‘The strided convolution refers to the specific\ncase in which, instead of applying the convolution operation for every\nlocation using a step size (or stride, s) of 1, different step sizes can be\nconsidered (Figure 17). Such an operation will produce a convolution\noutput with much fewer elements. Convolution blocks with s > 1 can\nbe found on CNN architectures as a way to decrease the feature sizes in\nintermediate layers.\n\nAtrous or dilated convolution Dilated, also called atrous, convolu-\ntion, is the convolution with kernels that have been dilated by inserting\nzero holes (a trous in French) between the non-zero values of a kernel.\nIn this case, an additional parameter (d) of the convolution operation is\nadded and it is changing the distance between the kernel elements. In\nessence, it is increasing the reach of the kernel but keeping the number\nof trainable parameters the same. For example, a dilated convolution\nwith a kernel size of 3 x 3 and a dilation rate of d = 2 would be sparsely\narranged on a 5 x 5 grid.\n\nTranspose convolution In certain circumstances, one needs not only\nto downsample the spatial dimensions of the input but also, usually at\na later stage of the network, apply an upsample operation. The most\nemblematic case is the task of image segmentation (see Chapter 13), in\nwhich a pixel-level classification is expected, and therefore the output of\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN 33\n\nthe neural network should have the same size as the input. In such cases,\nseveral upsampling operations are typically applied. The upsampling can\nbe achieved by a transpose convolution operation that will eventually\nincrease the size of the output. In details, the transpose convolution is\nperformed by dilating the input instead of the kernel before applying a\nconvolution operation. In this way, an input of size 5 x 5 will reach a\nsize of 10 x 10 after being dilated with d = 2. With proper padding and\nusing a kernel of size 3 x 3, the output will eventually double in size.\n\n4.4 Receptive field calculation\n\nIn the context of deep neural networks and specifically CNNs, the term\nreceptive field is used to define the proportion of the input that produces\na specific feature. For example, a CNN that takes an image as input and\napplies only a single convolution operation with a kernel size of 3 x 3\nwould have a receptive field of 3 x 3. This means that for each pixel of\nthe first feature map, a 3 x 3 region of the input would be considered.\nNow, if another layer were to be added, with again 3 x 3 size, then the\nreceptive field of the new feature map with respect to the CNN’s input\nwould be 5 x 5. In other words, the proportion of the input that is usec\nto calculate each element of the feature map of the second convolution\nlayer increases.\n\nCalculating the receptive field at different parts of a CNN is crucia.\nwhen trying to understand the inner workings of a specific architecture.\nFor instance, a CNN that is designed to take as an input an image of size\n256 x 256 and that requires information from all parts of it should have\na receptive field close to the size of the input. The receptive field can\nbe influenced by all the different convolution parameters and down/up-\nsampling operations described in the previous section. A comprehensive\npresentation of mathematical derivations for calculating receptive fields\nfor CNNs is given in [52].\n\n4.5 Classical convolutional neural network archi-\ntectures\n\nIn the last decades, a variety of convolutional neural network architec-\ntures have been proposed. In this chapter, we cover only a few classical\narchitectures for classification and regression. Note that classification\nand regression can usually be performed with the same architecture, just\nchanging the loss function (e.g. cross-entropy for classification, mean\nsquared error for regression). Architectures for other tasks can be found\nin other chapters.\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n34 Vakalopoulou et al.\n\nYO<sgO-\n0720-\nOo\n\n60-0000\n\nInput image Convolution Pooling Convolution Pooling\n+\n\n+\nNon-linearity Non-linearity Flatten Fully\n\nQe a7 connected\nVas a\n\nFeature learning Classification\n\nFigure 18: A basic CNN architecture. Classically, it is composed of\ntwo main parts. The first one, using convolutional operations, performs\nfeature learning. The features are then flattened and fed into a set of\nfully connected layers (i.e. a multilayer perceptron), which performs the\nclassification or the regression task.\n\nA basic CNN architecture Let us start with the most simple CNN,\nwhich is actually very close to the original one proposed by Le Cun et\nal. [53], sometimes called “LeNet”. Such architecture is typically com-\nposed of two parts: the first one is based on convolutional operations and\nlearns the features for the image, and the second part flattens the fea-\nures and inputs them to a set of fully connected layers (in other words,\na multilayer perceptron) for performing the classification/regression (see\nillustration in Figure 18). Note that, of course, the whole network is\n‘rained end-to-end: the two parts are not trained independently. In the\nfirst part, one combines a series of blocks composed of a convolution op-\neration (possibly strided and/or dilated), a non-linear activation function\nfor instance, a ReLU), and a pooling operation. It is often a good idea\n‘o include a drawing of the different layers of the chosen architecture.\nUnfortunately, there is no harmonized format for such a description. An\nexample is provided in Figure 19.\n\nOne of the first CNN architectures that follow this paradigm is the\nAlexNet architecture [54]. AlexNet was one of the first papers that em-\npirically indicated that the ReLU activation function makes the conver-\ngence of CNNs faster compared to other non-linearities such as the tanh.\nMoreover, it was the first architecture that achieved a top-5 error rate\nof 18.2% on the ImageNet dataset, outperforming all the other methods\non this benchmark by a huge margin (about 10%). Prior to AlexNet,\nbest-performing methods were using (very sophisticated) pre-extracted\nfeatures and classical machine learning. After this advance, deep learning\nin general and CNNs, in particular, became very active research direc-\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN 35\n\n1\n1\n\nHERE convotutionat ayer\nTI) batch normatization\nRet\n\nHE vecmaxrooiing\nHE Fiatten\n\nHEI Futty connected layer\n\n1, stride:\n1\n1, stride\n\n2x2x2, stride=\n2x2x2, stride=1\nOutput neurons=2\n\n16@3x3x3, pad\n16@3x3x3, pad=:\n\nu\n&\nry\nfot\na\n©\na\n\nFigure 19: A drawing describing a CNN architecture. Classically, it is\ncomposed of two main parts. Here 16@3 x 3 x 3 means that 16 features\nwith a 3 x 3 x 3 convolution kernel will be computed. For the pooling\noperation, the kernel size is also mentioned (2 x 2). Finally, the stride\nis systematically indicated.\n\ntions to address different computer vision problems. This resulted in\nthe introduction of a variety of architectures such as VGG16 [55] that\nreported a 7.3% error rate on ImageNet, introducing some changes such\nas the use of smaller kernel filters. Following these advances, and even\nif there were a lot of different architectures proposed during that period,\none could mention the Inception architecture [56], which was one of the\ndeepest architectures of that period and which further reduced the error\nrate on ImageNet to 6.7%. One of the main characteristics of this archi-\ntecture was the inception modules, which applied multiple kernel filters\nof different sizes at each level of the architecture. To solve the prob-\nlem of vanishing gradients, the authors introduced auxiliary classifiers\nconnected to intermediate layers, expecting to encourage discrimination\nin the lower stages in the classifier, increasing the gradient signal that\ngets propagated back, and providing additional regularization. During\ninference, these classifiers were completely discarded.\n\nIn the following section, some other recent and commonly used CNN\narchitectures, especially for medical applications, will be presented.\n\nResNet One of the most commonly used CNN architectures, even to-\nday, is the ResNet [57]. ResNet reduced the error rate on ImageNet to\n3.6%, while it was the first deep architecture that proposed novel concepts\non how to gracefully go deeper than a few dozen of layers. In particular,\nthe authors introduced a deep residual learning framework. The main\nidea of this residual learning is that instead of learning the desired un-\nderlying mapping of each network level, they learn the residual mapping.\nMore formally, instead of learning the H(x) mapping after the convolu-\ntional and non-linear layers, they fit another mapping of F(a) = H(#)—2x\non which the original mapping is recast into F(x)+a. Feedforward neural\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n36 Vakalopoulou et al.\n\nnetworks can realize this mapping with “shortcut connections” by simply\nperforming identity mapping, and their outputs are added to the outputs\nof the stacked layers. Such identity connections add neither additional\ncomplexity nor parameters to the network, making such architectures\nextremely powerful.\n\nDifferent ResNet architectures have been proposed even in the origi-\nnal paper. Even though the depth of the network is increased with the\nadditional convolutions, especially for the 152-layer ResNet (11.3 billion\nfloating point operations), it still has lower complexity (i.e. fewer parame-\nters) than VGG-16/19 networks. Currently, different layered-size ResNet\narchitectures pre-trained on ImageNet are used as backbones for vari-\nous problems and applications, including medical imaging. Pre-trained\nResNet models, even if they are 2D architectures, are commonly used on\nhistopathology [58, 59], chest X-ray [60] or even brain imaging [61, 62],\nwhile the way that such pre-trained networks work for medical applica-\ntions gathered the attention of different studies such as [63]. However, it\nshould be noted that networks pre-trained on ImageNet are not always\nefficient for medical imaging tasks, and there are cases where they per-\nform poorly, much lower than simpler CNNs trained from scratch [64].\nNevertheless, a pre-trained ResNet is very often a good idea to use for a\nfirst try in a given application. Finally, there was an effort from the med-\nical community to train 3D variations of ResNet architectures on a large\namount of 3D medical data and release the pre-trained models. Such\nan effort is presented in [65] in which the authors trained and released\ndifferent 3D ResNet architectures trained on different publicly available\n3D datasets, including different anatomies such as the brain, prostate,\nliver, heart and pancreas.\n\nEfficientNet A more recent CNN architecture that is worth mention-\ning in this section is the recently presented EfficientNet [66]. Efficient Nets\nare a family of neural networks that are balancing all dimensions of the\nnetwork (width/depth/resolution) automatically. In particular, the au-\nthors propose a simple yet effective compound scaling method for obtain-\ning these hyperpameters. In particular, the main compound coefficient\n¢ uniformly scales network width, depth, and resolution in a principled\nway: depth = a®, width = 8%, resolution = y? st. a+ 62-7? & 2,\na>1,6 > 1,7 = 1. In this formulation, the parameters a, 8,7 are\nconstants, and a small grid search can determine them. This grid search\nresulted in 8 different architectures presented in the original paper. Ef\nficientNet is used more and more for medical imaging tasks, as can be\nseen in multiple recent studies [67, 68, 69].\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN 37\n\n>-di\n\nFigure 20: The general principle of a denoising autoencoder. It aims\nat learning of a low-dimensional representation (latent space) z of the\ntraining data. The learning is done by aiming to provide a faithful recon-\nstruction & of the input data «.\n\n5. Autoencoders\n\nAn autoencoder is a type of neural network that can learn a compressed\nrepresentation (called the latent space representation) of the training\ndata. As opposed to the multilayer perceptrons and CNNs seen until\nnow that are used for supervised learning, autoencoders have widely been\nused for unsupervised learning, with a wide range of applications. The\narchitecture of autoencoders is composed of a contracting path (called the\nencoder), which will transform the input into a lower-dimensional repre-\nsentation, and an expanding path (called the decoder), which will aim at\nreconstructing the input as well as possible from the lower-dimensional\nrepresentation (see Figure 20).\nThe loss is usually the ¢) loss and the cost function is then:\n\nJ(0,6) = d je — Do(Eg(x))I03 (29)\n\nwhere Ey is the encoder (and @ its parameters) and Dg is the decoder\n(and @ its parameters). Note that, in Figure 20, De(Ey(x)) is denoted\nas &. More generally, one can write:\n\nJ(9,Q) = Eentnee [d(#, Do(Eg(a)))] (30)\n\nwhere [rep is the reference distribution that one is trying to approximate,\nand d is the reconstruction function. When prep is the empirical distri-\nbution of the training set and d is the 2 norm, Equation 30 is equivalent\nto Equation 29.\n\nMany variations of autoencoders exist, to prevent autoencoders from\nlearning the identity function and to improve their ability to capture\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n38 Vakalopoulou et al.\n\nimportant information and learn richer representations. Among them,\nsparse autoencoders offer an alternative method for introducing an infor-\nmation bottleneck without requiring a reduction in the number of nodes\nat the hidden features. This is done by constructing the loss function\nsuch that it penalizes activations within a layer. This is achieved by en-\nforcing sparsity in the network and encouraging it to learn an encoding\nand decoding which relies only on activating a small number of neurons.\nThis sparsity is enforced in two main ways, an ¢, regularization on the\nparameters of the network or a Kullback-Leibler divergence, which is a\nmeasure of the difference between two probability distributions. More in-\nformation about sparse autoencoders could be found in [70]. Moreover,\na quite common type of autoencoders is the denoising autoencoders [71],\non which the model is tasked with reproducing the input as closely as\npossible while passing through some sort of information bottleneck (Fig-\nure 20). This way, the model is not able to simply develop a mapping that\nmemorizes the training data, but rather learns a vector field for mapping\nthe input data towards a lower dimensional manifold. One should note\nhere that the vector field is typically well-behaved in the regions where\nthe model has observed data during training. In out-of-distribution data,\nthe reconstruction error is both large and does not always point in the\ndirection of the true distribution. This observation makes these networks\nquite popular for anomaly detection in medical data [72]. Additionally,\ncontractive autoencoders [73] are other variants of this type of models,\nadding the contractive regularization loss to the standard autoencoder\nloss. Intuitively, it forces very similar inputs to have a similar encoding\nand in particular, it requires the derivative of the hidden layer activations\nto be small with respect to small changes in the input. The denoising\nautoencoders can be understood as a variation of the contractive autoen-\ncoder. In the limit of small Gaussian noise, the denoising autoencoders\nmake the reconstruction error resistant to finite-sized input perturba-\ntions, while the contractive autoencoders make the extracted features\nresistant to small input perturbations.\n\nDepending on the input type, different autoencoder architectures\ncould be designed. In particular, when the inputs are images, the encoder\nand the decoder are classically composed of convolutional blocks. The\ndecoder uses, for instance, transpose convolutions to perform the expan-\nsion. Finally, the addition of skip connections has led to the U-Net [74]\narchitectures that are commonly used for segmentation purposes. Seg-\nmentation architectures will be more extensively described in Chapter 13.\nFinally, variational autoencoders, which rely on a different mathematical\nformulation, are not covered in the present chapter and are presented,\ntogether with other generative models, in Chapter 5.\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN 39\n\n6. Conclusion\n\nDeep learning is a very fast involving field, with numerous still unan-\nswered theoretical questions. However, deep learning-based models have\nbecome the state of the art methods for a variety of fields and tasks. In\nthis chapter, we presented the basic principles of deep learning, covering\nboth perceptrons and convolutional neural networks. All architectures\nwere feedforward and recurrent networks are covered in Chapter 4. Gen-\nerative adversarial networks are covered in Chapter 5, along with other\ngenerative models. Chapter 6 presents a recent class of deep learning\nmethods, which does not use convolutions, and that are called transform-\ners. Finally, throughout the other chapters of the book, different deep\nlearning architectures are presented for various types of applications.\n\nAcknowledgments\n\nThis work was supported in part by the French government under man-\nagement of Agence Nationale de la Recherche as part of the “Investisse-\nments d’avenir” program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA\nInstitute), reference ANR-10-IAIHU-06 (Institut Hospitalo- Universitaire\nICM) and ANR-21-CE45-0007 (Hagnodice).\n\nReferences\n\n1] Rosenblatt F (1957) The perceptron, a per- {8] Hochreiter S$, Schmidhuber J (1997) Long\nceiving and recognizing automaton Project short-term memory. Neural computation\nPara. Cornell Aeronautical Laboratory 9(8):1735-1780\n\n2] Minsky M, Papert S (1969) Perceptron: an\n\ns ¢ A {9] Hinton GE, Osindero S, Teh YW (2006) A\nintroduction to computational geometry\n\nfast learning algorithm for deep belief nets.\n\n3] Minsky ML, Papert SA (1988) Perceptrons: Neural computation’ 18(7):1527-1554\n\nexpanded edition\n{10] Hinton GE (2007) Learning multiple layers\n\n4] Linnainmaa S (1976) Taylor expansion of of representation. Trends in cognitive sci-\n\nthe accumulated rounding error. BIT Nu- ences 11(10):428-434\n\nmerical Mathematics 16(2):146-160\n\n{11] Deng J, Dong W, Socher R, Li LJ, Li K,\nFei-Fei L (2009) Imagenet: A large-scale hi-\nerarchical image database. In: 2009 IEEE\nconference on computer vision and pattern\nrecognition, Ieee, pp 248-255\n\n5] Werbos PJ (1982) Applications of advances\nin nonlinear sensitivity analysis. In: Sys-\ntem modeling and optimization, Springer,\npp 762-770\n\n6] Rumelhart DE, Hinton GE, Williams RJ\n(1986) Learning representations by back- [12] Bergstra J, Bastien F, Breuleux O, Lamblin\n\npropagating errors. nature 323(6088):533- P, Pascanu R, Delalleau O, Desjardins G,\n\n536 Warde-Farley D, Goodfellow I, Bergeron A,\n\net al (2011) Theano: Deep learning on gpus\n\n7] Le Cun Y= (1985) Une  procédure with python. In: NIPS 2011, BigLearning\n\nd’apprentissage pour réseau a seuil as- Workshop, Granada, Spain, Citeseer, vol 3,\nsymétrique. Cognitiva 85:599-604 pp 1-48\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n40\n\n[13]\n\n(14]\n\n[15]\n\n[16]\n\nued\n\n18\n\n19\n\n20\n\n21\n\n22)\n\n[23]\n\n[24]\n\n[25]\n\nJia Y, Shelhamer E, Donahue J, Karayev S,\nLong J, Girshick R, Guadarrama §, Darrell\nT (2014) Caffe: Convolutional architecture\nfor fast feature embedding. In: Proceedings\nof the 22nd ACM international conference\non Multimedia, pp 675-678\n\nAbadi M, Agarwal A, Barham P, Brevdo\nE, Chen Z, Citro C, Corrado GS, Davis A,\nDean J, Devin M, et al (2016) Tensorflow:\nLarge-scale machine learning on heteroge-\nneous distributed systems. arXiv preprint\narXiv:160304467\n\nChollet F, et al (2015) Keras. URL https:\n//github.com/fchollet/keras\n\nPaszke A, Gross S, Massa F, Lerer A,\nBradbury J, Chanan G, Killeen T, Lin\nZ, Gimelshein N, Antiga L, et al (2019)\nPytorch: An imperative style, high-\nperformance deep learning library. Ad-\nvances in neural information processing sys-\ntems 32\n\nHebb DO (1949) The Organization of Be-\nhavior: A Psychological Theory. Wiley New\nYork\n\nCybenko G (1989) Approximations by su-\nperpositions of a sigmoidal function. Math-\nematics of Control, Signals and Systems\n2:183-192\n\nHornik K, Stinchcombe M, White H\n(1989) Multilayer feedforward networks are\nuniversal approximators. Neural networks\n2(5):359-366\n\nMhaskar HN (1996) Neural networks for op-\ntimal approximation of smooth and analytic\nfunctions. Neural computation 8(1):164—177\n\nPinkus A (1999) Approximation theory of\nthe mlp model in neural networks. Acta nu-\nmerica 8:143-195\n\nPoggio T, Mhaskar H, Rosasco L, Miranda\nB, Liao Q (2017) Why and when can\ndeep-but not shallow-networks avoid the\ncurse of dimensionality: a review. Interna-\ntional Journal of Automation and Comput-\ning 14(5):503-519\n\nRolnick D, Tegmark M (2017) The power of\ndeeper networks for expressing natural func-\ntions. arXiv preprint arXiv:170505502\n\nGoodfellow I, Bengio Y, Courville A (2016)\nDeep learning. MIT press\n\nCover TM (1965) Geometrical and statisti-\ncal properties of systems of linear inequal-\nities with applications in pattern recogni-\ntion. IEEE transactions on electronic com-\nputers 3:326-334\n\n[26]\n\n[27]\n\n[28]\n\n[29]\n\n[30]\n\n[31]\n\n[32]\n\n[33]\n\n[34]\n\n[35]\n\n[36]\n\nVakalopoulou et al.\n\nGlorot X, Bordes A, Bengio Y (2011) Deep\nsparse rectifier neural networks. In: Pro-\nceedings of the fourteenth international con-\nference on artificial intelligence and statis-\ntics, JMLR Workshop and Conference Pro-\nceedings, pp 315-323\n\nKrizhevsky A, Sutskever I, Hinton GE\n(2012) Imagenet classification with deep\nconvolutional neural networks. Advances in\nneural information processing systems 25\n\nHein M, Andriushchenko M, Bitterwolf\nJ (2019) Why relu networks yield high-\nconfidence predictions far away from the\ntraining data and how to mitigate the prob-\nlem. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pat-\ntern Recognition, pp 41-50\n\nMaas AL, Hannun AY, Ng AY, et al (2013)\nRectifier nonlinearities improve neural net-\nwork acoustic models. In: Proc. icml, At-\nlanta, Georgia, USA, vol 30, p 3\n\nHe K, Zhang X, Ren S, Sun J (2015) Delving\ndeep into rectifiers: Surpassing human-level\nperformance on imagenet classification. In:\nProceedings of the IEEE international con-\nference on computer vision, pp 1026-1034\n\nRamachandran P, Zoph B, Le QV (2017)\nSearching for activation functions. arXiv\npreprint arXiv:171005941\n\nDauphin YN, Pascanu R, Gulcehre C, Cho\nK, Ganguli S, Bengio Y (2014) Identifying\nand attacking the saddle point problem in\nhigh-dimensional non-convex optimization.\nAdvances in neural information processing\nsystems 27\n\nBottou L (2010) Large-scale machine learn-\ning with stochastic gradient descent. In:\nProceedings of COMPSTAT’2010, Springer,\npp 177-186\n\nAllen-Zhu Z, Li Y, Song Z (2019) A con-\nvergence theory for deep learning via over-\nparameterization. In: International Confer-\nence on Machine Learning, PMLR, pp 242—\n252\n\nBaydin AG, Pearlmutter BA, Radul AA,\nSiskind JM (2018) Automatic differentia-\ntion in machine learning: a survey. Journal\nof Marchine Learning Research 18:1—43\n\nPrechelt L (1998) Early stopping-but when?\nIn: Neural Networks: Tricks of the trade,\nSpringer, pp 55-69\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\nDeep learning: basics and CNN\n\n[37]\n\n[38]\n\n39)\n\n40\n\nAL\n\n42\n\n44\n\n46\n\nAT\n\n48\n\nReed R, MarksII RJ (1999) Neural\nsmithing: supervised learning in feed-\nforward artificial neural networks. Mit\nPress\n\nGlorot X, Bengio Y (2010) Understanding\nthe difficulty of training deep feedforward\nneural networks. In: Proceedings of the thir-\nteenth international conference on artificial\nintelligence and statistics, JMLR Workshop\nand Conference Proceedings, pp 249-256\n\nSrivastava N, Hinton G, Krizhevsky A,\nSutskever I, Salakhutdinoy R_ (2014)\nDropout: a simple way to prevent neural\nnetworks from overfitting. The journal of\nmachine learning research 15(1):1929-1958\n\nDeng L (2012) The mnist database of hand-\nwritten digit images for machine learning\nresearch. IEEE Signal Processing Magazine\n29(6):141-142\n\nPérez-Garcfa F, Sparks R, Ourselin S (2021)\nTorchio: a python library for efficient\nloading, preprocessing, augmentation and\npatch-based sampling of medical images in\ndeep learning. Computer Methods and Pro-\ngrams in Biomedicine 208:106236\n\nloffe S, Szegedy C (2015) Batch normal-\nization: Accelerating deep network training\nby reducing internal covariate shift. In: In-\nternational conference on machine learning,\nPMLR, pp 448-456\n\nBrock A, De S, Smith SL, Simonyan K\n(2021) High-performance large-scale image\nrecognition without normalization. In: In-\nternational Conference on Machine Learn-\ning, PMLR, pp 1059-1071\n\nRuder S (2016) An overview of gradi-\nent descent optimization algorithms. arXiv\npreprint arXiv:160904747\n\nPolyak BT (1964) Some methods of speed-\ning up the convergence of iteration meth-\nods. Ussr computational mathematics and\nmathematical physics 4(5):1-17\n\nQian N (1999) On the momentum term in\ngradient descent learning algorithms. Neu-\nral networks 12(1):145-151\n\nDuchi J, Hazan E, Singer Y (2011) Adaptive\nsubgradient methods for online learning and\nstochastic optimization. Journal of machine\nlearning research 12(7)\n\nKingma DP, Ba J (2014) Adam: A method\nfor stochastic optimization. arXiv preprint\narXiv:14126980\n\n49)\n\n50)\n\n51\n\n[52]\n\n[53]\n\n[55]\n\n[56]\n\n[57]\n\n[58]\n\n41\n\nLiu L, Jiang H, He P, Chen W, Liu X, Gao\nJ, Han J (2019) On the variance of the\nadaptive learning rate and beyond. arXiv\npreprint arXiv:190803265\n\nZhang M, Lucas J, Ba J, Hinton GE (2019)\nLookahead optimizer: k steps forward, 1\nstep back. Advances in neural information\nprocessing systems 32\n\nFukushima K, Miyake S (1982) Neocog-\nnitron: A self-organizing neural network\nmodel for a mechanism of visual pattern\nrecognition. In: Competition and coopera-\ntion in neural nets, Springer, pp 267-285\n\nAraujo A, Norris W, Sim J (2019) Com-\nputing receptive fields of convolutional neu-\nral networks. Distill DOI 10.23915/distill.\n00021, https: //distill.pub/2019/computing-\nreceptive-fields\n\nLeCun Y, Boser B, Denker JS, Henderson\nD, Howard RE, Hubbard W, Jackel LD\n(1989) Backpropagation applied to hand-\nwritten zip code recognition. Neural com-\nputation 1(4):541-551\n\nKrizhevsky A, Sutskever I, Hinton GE\n(2012) Imagenet classification with deep\nconvolutional neural networks. In: Pereira\nF, Burges C, Bottou L, Weinberger K\n(eds) Advances in Neural Information Pro-\ncessing Systems, Curran Associates, Inc.,\nvol 25, URL https://proceedings.\nneurips.cc/paper/2012/file/\nc399862d3b9d6b76c8436e924a68c45b-Paper.\npdf\n\nSimonyan K, Zisserman A (2014) Very deep\nconvolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:14091556\n\nSzegedy C, Liu W, Jia Y, Sermanet P, Reed\nS, Anguelov D, Erhan D, Vanhoucke V, Ra-\nbinovich A (2015) Going deeper with convo-\nlutions. In: Proceedings of the IEEE confer-\nence on computer vision and pattern recog-\nnition, pp 1-9\n\nHe K, Zhang X, Ren S, Sun J (2016) Deep\nresidual learning for image recognition. In:\nProceedings of the IEEE conference on com-\nputer vision and pattern recognition, pp\n770-778\n\nLu MY, Williamson DF, Chen TY, Chen\nRJ, Barbieri M, Mahmood F (2021) Data-\nefficient and weakly supervised computa-\ntional pathology on whole-slide images. Na-\nture biomedical engineering 5(6):555-570\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n42\n\n(59)\n\n60)\n\n61\n\n62\n\n[63]\n\n[64]\n\n[65]\n\n[66]\n\nBenkirane H, Vakalopoulou M,\nChristodoulidis S$, Garberis IJ, Michiels S,\nCournéde PH (2022) Hyper-adac: Adaptive\nclustering-based hypergraph representation\nof whole slide images for survival analysis.\nIn: Machine Learning for Health, PMLR,\npp 405-418\n\nHorry MJ, Chakraborty S, Paul M, Ulhaq\nA, Pradhan B, Saha M, Shukla N (2020)\nX-ray image based covid-19 detection using\npre-trained deep learning models. Engineer-\ning Archive\n\nLi JP, Khan S, Alshara MA, Alotaibi RM,\nMawuli C, et al (2022) Dacbt: deep learn-\ning approach for classification of brain tu-\nmors using mri data in iot healthcare envi-\nronment. Scientific Reports 12(1):1—-14\n\nNandhini I, Manjula D, Sugumaran V\n(2022) Multi-class brain disease classifica-\ntion using modified pre-trained convolu-\ntional neural networks model with substan-\ntial data augmentation. Journal of Medical\nImaging and Health Informatics 12(2):168—\n183\n\nRaghu M, Zhang C, Kleinberg J, Bengio S\n(2019) Transfusion: Understanding transfer\nlearning for medical imaging. Advances in\nneural information processing systems 32\n\nWen J, Thibeau-Sutre E, Diaz-Melo M,\nSamper-Gonzalez J, Routier A, Bottani S,\nDormont D, Durrleman S, Burgos N, Col-\nliot O (2020) Convolutional neural net-\nworks for classification of alzheimer’s dis-\nOverview and reproducible evalua-\ntion. Medical image analysis 63:101694\n\nease:\n\nChen S, Ma K, Zheng Y (2019) Med3d:\nTransfer learning for 3d medical image anal-\nysis. arXiv preprint arXiv:190400625\n\nTan M, Le Q (2019) Efficientnet: Rethink-\ning model scaling for convolutional neural\nnetworks. In: International conference on\nmachine learning, PMLR, pp 6105-6114\n\n[67]\n\n[68]\n\n[69]\n\n[70]\n\n(71)\n\n[72]\n\n[73]\n\n[74]\n\nVakalopoulou et al.\n\nWang J, Liu Q, Xie H, Yang Z, Zhou H\n(2021) Boosted efficientnet: Detection of\nlymph node metastases in breast cancer us-\ning convolutional neural networks. Cancers\n13(4):661\n\nOloko-Oba M, Viriri S (2021) Ensemble of\nefficientnets for the diagnosis of tuberculo-\nsis. Computational Intelligence and Neuro-\nscience 2021\n\nAli K, Shaikh ZA, Khan AA, Laghari AA\n(2021) Multiclass skin cancer classification\nusing efficientnets—a first step towards pre-\nventing skin cancer. Neuroscience Informat-\nics p 100034\n\nNg A, et al (2011) Sparse autoencoder.\nCS294A Lecture notes 72(2011):1-19\n\nVincent P, Larochelle H, Bengio Y, Man-\nzagol PA (2008) Extracting and compos-\ning robust features with denoising autoen-\ncoders. In: Proceedings of the 25th interna-\ntional conference on Machine learning, pp\n1096-1103\n\nBaur C, Denner S, Wiestler B, Navab N,\nAlbarqouni S (2021) Autoencoders for un-\nsupervised anomaly segmentation in brain\nmr images: a comparative study. Medical\nImage Analysis 69:101952\n\nSalah R, Vincent P, Muller X, et al (2011)\nContractive auto-encoders: Explicit invari-\nance during feature extraction. In: Proc. of\nthe 28th International Conference on Ma-\nchine Learning, pp 833-840\n\nRonneberger O, Fischer P, Brox T (2015) U-\nnet: Convolutional networks for biomedical\nimage segmentation. In: International Con-\nference on Medical image computing and\ncomputer-assisted intervention, Springer,\npp 234-241\n\nMachine Learning for Brain Disorders, Chapter 3\n\n\n",
  "structured_data": {
    "dates": [
      "10",
      "11",
      "2010",
      "12",
      "13",
      "14",
      "15",
      "16",
      "17",
      "21",
      "71",
      "41",
      "18",
      "19",
      "20",
      "21",
      "21",
      "21",
      "11",
      "22",
      "23",
      "11",
      "24",
      "1965",
      "25",
      "10",
      "11",
      "28",
      "26",
      "27",
      "25",
      "10",
      "28",
      "29",
      "30",
      "12",
      "31",
      "31",
      "13",
      "10",
      "29",
      "11",
      "12",
      "71",
      "13",
      "14",
      "15",
      "14",
      "14",
      "24",
      "24",
      "32",
      "16",
      "32",
      "33",
      "15",
      "16",
      "24",
      "24",
      "17",
      "34",
      "17",
      "1970",
      "18",
      "18",
      "14",
      "16",
      "35",
      "19",
      "20",
      "19",
      "36",
      "24",
      "20",
      "37",
      "24",
      "24",
      "38",
      "20",
      "39",
      "21",
      "40",
      "41",
      "2127",
      "12",
      "212",
      "2/212",
      "1212",
      "33",
      "33",
      "42",
      "43",
      "22",
      "44",
      "45",
      "46",
      "21",
      "24",
      "23",
      "47",
      "22",
      "38",
      "24",
      "24",
      "48",
      "24",
      "49",
      "25",
      "50",
      "51",
      "2012",
      "27",
      "25",
      "26",
      "26",
      "27",
      "28",
      "10",
      "27",
      "10",
      "11",
      "12",
      "11",
      "28",
      "12",
      "02",
      "03",
      "20",
      "130",
      "431",
      "432",
      "433",
      "29",
      "13",
      "13",
      "14",
      "30",
      "14",
      "256",
      "10",
      "10",
      "31",
      "15",
      "16",
      "15",
      "32",
      "4445",
      "17",
      "16",
      "17",
      "13",
      "33",
      "10",
      "10",
      "256",
      "256",
      "52",
      "34",
      "0720",
      "60-0000",
      "18",
      "53",
      "18",
      "19",
      "54",
      "18",
      "10",
      "35",
      "16",
      "16",
      "19",
      "16",
      "16",
      "55",
      "56",
      "57",
      "36",
      "152",
      "11",
      "16/19",
      "58",
      "59",
      "60",
      "61",
      "62",
      "63",
      "64",
      "65",
      "66",
      "62",
      "67",
      "68",
      "69",
      "37",
      "20",
      "20",
      "29",
      "20",
      "30",
      "30",
      "29",
      "38",
      "70",
      "71",
      "20",
      "72",
      "73",
      "74",
      "13",
      "39",
      "19",
      "0001",
      "10",
      "06",
      "21",
      "0007",
      "1957",
      "1997",
      "1735",
      "1780",
      "1969",
      "2006",
      "1988",
      "18",
      "1527",
      "1554",
      "10",
      "2007",
      "1976",
      "11",
      "10",
      "428",
      "434",
      "16",
      "146",
      "160",
      "11",
      "2009",
      "2009",
      "248",
      "255",
      "1982",
      "762",
      "770",
      "1986",
      "12",
      "323",
      "6088",
      "533",
      "536",
      "2011",
      "1985",
      "2011",
      "85",
      "599",
      "604",
      "1-48",
      "40",
      "13",
      "14",
      "15",
      "16",
      "18",
      "19",
      "20",
      "21",
      "22",
      "23",
      "24",
      "25",
      "2014",
      "675",
      "678",
      "2016",
      "2015",
      "2019",
      "32",
      "1949",
      "1989",
      "183",
      "192",
      "1989",
      "359",
      "366",
      "1996",
      "164",
      "177",
      "1999",
      "143",
      "195",
      "2017",
      "14",
      "503",
      "519",
      "2017",
      "2016",
      "1965",
      "326",
      "334",
      "26",
      "27",
      "28",
      "29",
      "30",
      "31",
      "32",
      "33",
      "34",
      "35",
      "36",
      "2011",
      "315",
      "323",
      "2012",
      "25",
      "2019",
      "41-50",
      "2013",
      "30",
      "2015",
      "1026",
      "1034",
      "2017",
      "2014",
      "27",
      "2010",
      "2010",
      "177",
      "186",
      "2019",
      "242",
      "252",
      "2018",
      "18",
      "43",
      "1998",
      "55-69",
      "37",
      "38",
      "39",
      "40",
      "42",
      "44",
      "46",
      "48",
      "1999",
      "2010",
      "249",
      "256",
      "2014",
      "15",
      "1929",
      "1958",
      "2012",
      "29",
      "141",
      "142",
      "2021",
      "208",
      "2015",
      "448",
      "456",
      "2021",
      "1059",
      "1071",
      "2016",
      "1964",
      "1-17",
      "1999",
      "12",
      "145",
      "151",
      "2011",
      "12",
      "2014",
      "49",
      "50",
      "51",
      "52",
      "53",
      "55",
      "56",
      "57",
      "58",
      "41",
      "2019",
      "2019",
      "32",
      "1982",
      "267",
      "285",
      "2019",
      "10",
      "2019",
      "1989",
      "541",
      "551",
      "2012",
      "25",
      "2012",
      "2014",
      "2015",
      "2016",
      "770",
      "778",
      "2021",
      "555",
      "570",
      "42",
      "59",
      "60",
      "61",
      "62",
      "63",
      "64",
      "65",
      "66",
      "2022",
      "405",
      "418",
      "2020",
      "19",
      "2022",
      "12",
      "14",
      "2022",
      "12",
      "168",
      "183",
      "2019",
      "32",
      "2020",
      "63",
      "2019",
      "2019",
      "6105",
      "6114",
      "67",
      "68",
      "69",
      "70",
      "71",
      "72",
      "73",
      "74",
      "2021",
      "13",
      "661",
      "2021",
      "2021",
      "2021",
      "2011",
      "72",
      "2011",
      "1-19",
      "2008",
      "1096",
      "1103",
      "2021",
      "69",
      "2011",
      "833",
      "840",
      "2015",
      "234",
      "241"
    ],
    "possible_ids": [
      "PRAIRIE",
      "160304467",
      "170505502",
      "171005941",
      "COMPSTAT",
      "106236",
      "160904747",
      "14126980",
      "190803265",
      "14091556",
      "101694",
      "190400625",
      "100034",
      "CS294A",
      "101952"
    ],
    "names": [],
    "organizations": [],
    "locations": []
  },
  "created_at": "2025-07-02T12:20:11.194452"
}